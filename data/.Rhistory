ls()
?ls()
read.table(file.choose(),sep=",",header=TRUE)
read.table(file.choose(),sep=",",header=TRUE)
t = table(Class)
install.packages(c("dplyr", "ggplot2", "nycflights13"))
read.table(file.choose(),sep=",",header=TRUE)
install.packages(c("dplyr", "ggplot2", "nycflights13"))
dim(flights)
library(nycflights13)
dim(flights)
head(flights)
tail(flights)
filter(flights, month==1, day==1)
filter(flights,month==1,day==1)
f=filter(flights,month==1,day==1)
f=filter(flights,month==1,day==1)
install.packages("dplyr")
install.packages("dplyr")
install.packages("nycflights13")
install.packages("nycflights13")
library(nycflights13)
f=filter(flights,month==1,day==1)
x <- c(0,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
# Calculating and plotting entropy for the binary case
?log2
# define function to measure the performance of a classification model
entropy <- function(p){
ifelse(p==0 | p==1, 0, -(p*log2(p)+(1-p)*log2(1-p)))
}
entropy(0.5)
entropy(0.99999)
entropy(1)
entropy(0)
x <- c(0,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
y <- entropy(x)
plot(x,y, type="b", xlab="p", ylab="Entropy")
IG <- function(p, p1, p2, prop1){
entropy(p) - (prop1*entropy(p1)+(1-prop1)*entropy(p2))
}
IG(0.53, 0.92, 0.24, 0.43)
# We will now use the rpart package for decision tree classification
pkgs <- c("rpart", "rpart.plot", "party", "randomForest", "e1071")
#install.packages(pkgs,lib="C:/Rick/R/Rlib",depend=TRUE)
install.packages(pkgs, depend=TRUE)
?install.packages
?sample
?table
?rpart
# CUSTOMIZE DATA FILE: Define website that provides data file
loc <- "http://archive.ics.uci.edu/ml/machine-learning-databases/"
# CUSTOMIZE DATA FILE: Define data file
ds <- "breast-cancer-wisconsin/breast-cancer-wisconsin.data"
# Paste converts its arguments (via as.character) to character strings, and concatenates them (separating them by the string given by sep)
url <- paste(loc, ds, sep="")
# CUSTOMIZE DATA FILE: read.table reads a file in table format and creates a data frame from it
breast <- read.table(url, sep=",", header=FALSE, na.strings="?")
# CUSTOMIZE DATA FILE: Define column names
# CUSTOMIZE DATA FILE: The categorical dependent variable is called target
names(breast) <- c("ID", "clumpThickness", "sizeUniformity",
"shapeUniformity", "marginalAdhesion",
"singleEpithelialCellSize", "bareNuclei",
"blandChromatin", "normalNucleoli", "mitosis", "target")
# Define data frame containing all columns except first column
df <- breast[-1]
# CUSTOMIZE DATA FILE: factor relabels categorical values for a categorical variable
# CUSTOMIZE DATA FILE: The categorical dependent variable is called target
df$target <- factor(df$target, levels=c(2,4),
labels=c("benign", "malignant"))
# Define seed for random number generator
set.seed(1234)
# sample takes a random sample of the specified size from the elements of x
train <- sample(nrow(df), 0.7*nrow(df))
# Define training data frame using random sample of observations
df.train <- df[train,]
# Define validation data frame using all observations not in training data frame
df.validate <- df[-train,]
# CUSTOMIZE DATA: Table counts the observations for each categorical value of target
# CUSTOMIZE DATA: The categorical dependent variable is called target
table(df.train$target)
table(df.validate$target)
# Decision Tree
# library loads add-on packages
#library(rpart,lib="C:/Rick/R/Rlib")
library(rpart)
# Define seed for random number generator
set.seed(1234)
# Fit a recursive partitioning model
# CUSTOMIZE DATA: the first parameter target specifies the categorical dependent variable
# CUSTOMIZE DATA: The categorical dependent variable is called target
dtree <- rpart(target ~ ., data=df.train, method="class",
parms=list(split="information"))
# Summarize the decision tree including decision nodes and leaf nodes
# The decision tree nodes are described row by row, then left to right
summary(dtree)
# Display decision tree.  The true values follow the left branches.
plot(dtree);text(dtree)
# Display decision tree complexity parameter table which is a matrix of information on the optimal prunings based on a complexity parameter
# Identify CP that corresponds to the lowest xerror
dtree$cptable
# Plot complexity parameter table
plotcp(dtree)
# Determine CP that corresponds to the lowest xerror
# Get index of CP with lowest xerror
opt <- which.min(dtree$cptable[,"xerror"])
# get its CP value
cp <- dtree$cptable[opt, "CP"]
# Prune decision tree to decrease overfitting
dtree.pruned <- prune(dtree, cp)
# Display pruned decision tree.  The true values follow the left branches.
plot(dtree.pruned);text(dtree.pruned)
# class displays the object class
class(dtree$cptable)
# names displays the names of an object
names(dtree)
#library(rpart.plot,lib="C:/Rick/R/Rlib")
library(rpart.plot)
# Determine proportion of categorical dependent variable
# CUSTOMIZE DATA: The categorical dependent variable is called target
table(df.train$target)/nrow(df.train)
# prp plots an rpart model
prp(dtree.pruned, type=2, extra=104, fallen.leaves=TRUE, main="Decision Tree")
# predict evaluates the application of a model to a data frame
dtree.pred <- predict(dtree.pruned, df.validate, type="class")
# define classification matrix
# CUSTOMIZE DATA: The categorical dependent variable is called target
dtree.perf <- table(df.validate$target, dtree.pred, dnn=c("Actual", "Predicted"))
dtree.perf
# Random Forest
#library(randomForest,lib="C:/Rick/R/Rlib")
library(randomForest)
# Define seed for random number generator
set.seed(1234)
# Fit a random forest model
# The na.action=na.roughfix option replaces missing values on numeric variables with column medians and missing values on categorical variables with the modal category for that variable
# CUSTOMIZE DATA: The categorical dependent variable is called target
forest <- randomForest(target ~ ., data=df.train,
na.action=na.roughfix,
importance=TRUE)
# Display a summary for a random forest model
forest
# Display variable importance measures for a random forest model
importance(forest, type=2)
# predict evaluates the application of a model to a data frame
forest.pred <- predict(forest, df.validate)
# define classification matrix
# CUSTOMIZE DATA: The categorical dependent variable is called target
forest.perf <- table(df.validate$target, forest.pred,
dnn=c("Actual", "Predicted"))
forest.perf
source('C:/Users/Ryan Luu/Desktop/MSBA SUMMER \'19/BANA200A/Project 4/Classification Rev4.R')
setwd("~/GitHub/OCRugVARKS/data")
data <- read.csv("bank-full.csv", TRUE, ';')
#Packages
library(randomForest)
library(caret)
library(ggplot2)
ggplot(data, aes(job, fill = y)) + geom_bar() +
labs(title = "Frequency of Clients Subscribing to a Term Deposit",
x = "Response",
y = "Frequency",
fill = "Did the client subscribe a term deposit?")
ggplot(data, aes(y, fill = y)) + geom_bar() +
labs(title = "Frequency of Clients Subscribing to a Term Deposit",
x = "Response",
y = "Frequency",
fill = "Did the client subscribe a term deposit?")
set.seed(1234)
ind <- sample(2,nrow(data_no), replace = TRUE, prob = c(0.7,0.3))
train <- data_no[ind==1,]
test <- data_no[ind==2,]
ind <- sample(2,nrow(data_yes), replace = TRUE, prob = c(0.7,0.3))
train1 <- data_yes[ind==1,]
test1 <- data_yes[ind==2,]
train <- rbind(train, train1)
test <- rbind(test, test1)
data <- subset(data, select =-c(duration))
data_yes <- data[ which(data$y=='yes'),]
data_no <- data[ which(data$y=='no'),]
#Partitioning the dataset
set.seed(1234)
ind <- sample(2,nrow(data_no), replace = TRUE, prob = c(0.7,0.3))
train <- data_no[ind==1,]
test <- data_no[ind==2,]
ind <- sample(2,nrow(data_yes), replace = TRUE, prob = c(0.7,0.3))
train1 <- data_yes[ind==1,]
test1 <- data_yes[ind==2,]
train <- rbind(train, train1)
test <- rbind(test, test1)
data_yes$job / data$job
count_field(data_yes$job) / count_field(data$job)
count.fields(data_yes$job)
library(dplyr)
count(data_yes$job)
count(data_yes$job)
nrow(data_yes)
nrow(which(data$job == "housemaid"))
nrow(which(data$job == "retired"))
count(which(data$job == "retired"))
nrow(data$job)
nrow(data['job']=="retired")
nrow(data['job']=="housemaid")
nrow(data['rx']=="housemaid")
nrow(data['rx']="housemaid")
nrow(data[job]=="housemaid")
nrow(data['job']=="housemaid")
nrow(data['job']=="rx")
data$job
library(party)
tree <- ctree(y~., data = train)
plot(tree)
tree <- ctree(y~., data = train, ctree_control(mincriterion = 0.95, minsplit = 20))
plot(tree)
tree <- ctree(y ~ ., data = train, controls = ctree_control(minsplit= 50, minbucket = 20))
plot(tree)
tree <- ctree(y ~ ., data = train,
controls = ctree_control(maxsurrogate = 3))
plot(tree)
tree <- ctree(y~ marital + education + housing + loan, data = train, ctree_control(mincriterion = 0.95, minsplit = 20))
plot(tree)
tree <- ctree(y~ marital + education + housing + loan, data = train)
plot(tree)
varImpPlot(rf)
library(randomForest)
varImpPlot(rf)
rf <- randomForest(y~., data=train)
varImpPlot(rf)
tree <- ctree(y~ balance + month + age + day + job + poutcome + pdays + campaign + education, data = train)
plot(tree)
tree <- ctree(y~ balance + age + job + poutcome + pdays + campaign + education, data = train)
plot(tree)
tree <- ctree(y~ balance + age + job + poutcome + pdays + campaign, data = train)
plot(tree)
tree <- ctree(y~ balance + age + poutcome + pdays + campaign, data = train)
plot(tree)
partialPlot(rf, train, job, "yes")
varImpPlot(rf)
data <- subset(data, select =-c(duration))
data_yes <- data[ which(data$y=='yes' + data$job=='student'),]
data_no <- data[ which(data$y=='no' + data$job=='student'),]
data_yes <- data[ which(data$y=='yes' & data$job=='student'),]
data_no <- data[ which(data$y=='no' & data$job=='student'),]
View(data_yes)
set.seed(1234)
ind <- sample(2,nrow(data_no), replace = TRUE, prob = c(0.7,0.3))
train <- data_no[ind==1,]
test <- data_no[ind==2,]
ind <- sample(2,nrow(data_yes), replace = TRUE, prob = c(0.7,0.3))
train1 <- data_yes[ind==1,]
test1 <- data_yes[ind==2,]
train <- rbind(train, train1)
test <- rbind(test, test1)
#Remove Variable
rm(train1, test1)
summary(data$job)
summary(data_yes$job)
View(train)
View(data_yes)
rf <- randomForest(y~., data=train)
library(caret)
p1 <- predict(rf, train)
confusionMatrix(p1, train$y)
#Prediction & Confusion MAtrix - test data
p2 <- predict(rf, test)
confusionMatrix(p2, test$y)
library(randomForest)
varImpPlot(rf)
tree <- ctree(y~ balance + age + poutcome + pdays + campaign, data = train)
plot(tree)
tree <- ctree(y~., data = train)
plot(tree)
data <- subset(data, select =-c(duration))
data <- read.csv("bank-full.csv", TRUE, ';')
data <- subset(data, select =-c(duration))
data_yes <- data[ which(data$y=='yes' & data$age >= 60),]
data_no <- data[ which(data$y=='no' & data$age >= 60),]
View(data_yes)
data_yes <- data[ which(data$y=='yes' & data$age >= 60),]
data_no <- data[ which(data$y=='no' & data$age >= 60),]
#Partitioning the dataset
set.seed(1234)
ind <- sample(2,nrow(data_no), replace = TRUE, prob = c(0.7,0.3))
train <- data_no[ind==1,]
test <- data_no[ind==2,]
ind <- sample(2,nrow(data_yes), replace = TRUE, prob = c(0.7,0.3))
train1 <- data_yes[ind==1,]
test1 <- data_yes[ind==2,]
train <- rbind(train, train1)
test <- rbind(test, test1)
#Remove Variable
rm(train1, test1)
rf <- randomForest(y~., data=train)
library(caret)
p1 <- predict(rf, train)
confusionMatrix(p1, train$y)
p2 <- predict(rf, test)
confusionMatrix(p2, test$y)
library(randomForest)
varImpPlot(rf)
tree <- ctree(y~., data = train)
plot(tree)
View(train)
setwd("~/GitHub/OCRugVARKS/data")
data <- read.csv("bank-full.csv", TRUE, ';')
#Packages
library(randomForest)
library(caret)
library(ggplot2)
library(dplyr)
data <- subset(data, select =-c(duration))
data_yes <- data[ which(data$y=='yes'),]
data_no <- data[ which(data$y=='no'),]
set.seed(1234)
ind <- sample(2,nrow(data_no), replace = TRUE, prob = c(0.7,0.3))
train <- data_no[ind==1,]
test <- data_no[ind==2,]
ind <- sample(2,nrow(data_yes), replace = TRUE, prob = c(0.7,0.3))
train1 <- data_yes[ind==1,]
test1 <- data_yes[ind==2,]
train <- rbind(train, train1)
test <- rbind(test, test1)
#Remove Variable
rm(train1, test1)
data_age1 <- data[which(data$age>=18 & data$age <30),]
View(data_age1)
data_age2 <- data[which(data$age>=30 & data$age <60),]
data_age3 <- data[which(data$age>60)]
data_age3 <- data[which(data$age>60),]
summary(data_age1)
summary(data_age1)
summary(data_age1)
summary(data_age1$y)
summary(data_age1$y)
summary(data_age1$y)
summary(data_age1$y)
summary(data_age2$y)
summary(data_age3$y)
nrow(data[which(data$age>=18 & data$ <30),])
nrow(data[which(data$age>=18 & data$age <30),])
nrow(data_age1[which(data_age1$y=='yes'),])/
nrow(data_age1)
nrow(data_age1)
nrow(data_age1[which(data_age1$y=='yes'),])/nrow(data_age1)
nrow(data_age1[which(data_age1$y=='yes'),])/nrow(data_age1)
nrow(data_age2[which(data_age2$y=='yes'),])/nrow(data_age2)
nrow(data_age3[which(data_age3$y=='yes'),])/nrow(data_age3)
mean(test_no$balance)
mean(data$balance)
cor(data$age, data$y)
ggplot(data, aes(y, fill = y)) + geom_bar() +
labs(title = "Frequency of Clients Subscribing to a Term Deposit",
x = "Response",
y = "Frequency",
fill = "Did the client subscribe a term deposit?")
ggplot(data, aes(job, fill = y)) + geom_bar() +
labs(title = "Frequency of Clients Subscribing to a Term Deposit by Jobs",
x = "Response",
y = "Frequency",
fill = "Did the client subscribe a term deposit?")
ggplot(data, aes(job, fill = y)) + geom_bar(position = "fill") +
labs(title = "Frequency of Clients Subscribing to a Term Deposit by Ratio",
x = "Response",
y = "Frequency",
fill = "Did the client subscribe a term deposit?")
ggplot(data_age1, aes(y, fill = y)) + geom_bar() +
labs(title = "Frequency of Clients Subscribing to a Term Deposit (18-29)",
x = "Response",
y = "Frequency",
fill = "Did the client subscribe a term deposit?")
ggplot(data_age2, aes(y, fill = y)) + geom_bar() +
labs(title = "Frequency of Clients Subscribing to a Term Deposit (18-29)",
x = "Response",
y = "Frequency",
fill = "Did the client subscribe a term deposit?")
ggplot(data_age3, aes(y, fill = y)) + geom_bar() +
labs(title = "Frequency of Clients Subscribing to a Term Deposit (18-29)",
x = "Response",
y = "Frequency",
fill = "Did the client subscribe a term deposit?")
#Age Group 18-29
ggplot(data_age1, aes(y, fill = y)) + geom_bar() +
labs(title = "Frequency of Clients Subscribing to a Term Deposit (18-29)",
x = "Response",
y = "Frequency",
fill = "Did the client subscribe a term deposit?")
ggplot(data_age2, aes(y, fill = y)) + geom_bar() +
labs(title = "Frequency of Clients Subscribing to a Term Deposit (30-59)",
x = "Response",
y = "Frequency",
fill = "Did the client subscribe a term deposit?")
ggplot(data_age3, aes(y, fill = y)) + geom_bar() +
labs(title = "Frequency of Clients Subscribing to a Term Deposit (60+)",
x = "Response",
y = "Frequency",
fill = "Did the client subscribe a term deposit?")
ggplot(data_age3, aes(y, fill = y)) + geom_bar() +
geom_text(stat = 'count',aes(label =..count.., vjust = -0.2)) +
labs(title = "Frequency of Clients Subscribing to a Term Deposit (60+)",
x = "Response",
y = "Frequency",
fill = "Did the client subscribe a term deposit?")
summary(data_age1$y)
summary(data_age2$y)
summary(data_age3$y)
nrow(data_age1[which(data_age1$y=='yes'),])/nrow(data_age1)
nrow(data_age2[which(data_age2$y=='yes'),])/nrow(data_age2)
nrow(data_age3[which(data_age3$y=='yes'),])/nrow(data_age3)
